# -*- coding: utf-8 -*-
"""ai3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_DMHxyNRhlE5mjzOet1Xf-VddwgYp6__
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from google.colab import drive
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
import random
from random import randint

drive.mount('/content/drive')
df=pd.read_csv("/content/drive/MyDrive/Colab Files/creditcard.csv")

"""### **DATA PREPROCESSING**"""

#Engineering data
df = df.sample(frac=1)
df_fe = df
col = list(df_fe)
sc = StandardScaler()
amount = df['Amount'].values
df['Amount'] = sc.fit_transform(amount.reshape(-1, 1))
df.dropna()
# for col in columns:
#     df_fe = df_fe.fillna({col: df_fe[col].mean()})
x = np.array(df_fe[col])
y = np.array(df['Class'])

# # X is the feature matrix and y is the target variable
# over = SMOTE(sampling_strategy=0.1)
# under = RandomUnderSampler(sampling_strategy=0.5)
# steps = [('o', over), ('u', under)]
# pipeline = Pipeline(steps=steps)

# cnt2=0
# x_resampled, y_resampled = pipeline.fit_resample(x, y)
# for i in range(len(y_resampled)):
#   if(y_resampled[i]==1):
#     cnt2+=1
# print(cnt2)
# print(x_resampled.shape)
# x_resampled

smote = SMOTE(sampling_strategy=0.2)
x_resampled_orig, y_resampled_orig = smote.fit_resample(x, y)

"""### **RANDOM FOREST(FITNESS FUNCTION)**"""

def get_fitness(features):
    train_size = int(len(x_resampled_orig)*0.3)
    x_resampled_df = pd.DataFrame(x_resampled_orig, columns=['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount','Class'])
    x_resampled_subset = x_resampled_df[features].values
    x_resampled_new =x_resampled_subset
    # x_resampled=np.delete(x_resampled, 29, 1)
    x_test = x_resampled_new[train_size:]
    x_train = x_resampled_new[:train_size]
    y_test = y_resampled_orig[train_size:]
    y_train = y_resampled_orig[:train_size]

    #RF classifier
    rf_clf = RandomForestClassifier(max_depth=3)

    #Train RF classifier
    rf_clf.fit(x_train, y_train)

    # Evaluate the model on testing data
    test_preds = rf_clf.predict(x_test)

    # Step 5: Store the predictions
    preds = test_preds

    # Step 6: Evaluate the predictions using accuracy as the performance metric
    accuracy = accuracy_score(y_test, preds)
    return accuracy

"""### **GENETIC ALGORITHM**"""

tcol =np.delete(col, 30)
B = []
C = []
# Step 4: Generate initial population
np.random.shuffle(tcol)
#population = tcol[:16]

#Encoding
population=""
for i in range(30):
     k = random.randint(0, 1)
     population+= str(k);

# Step 5: Compute candidate feature vector
for i in range(2):


    # Step 6: Generate fitness value
    #fitness = get_fitness(population)
    features=[]
    for i in range(30):
        if(population[i]=='1'):
            features.append(col[i])
    fitness=get_fitness(features)


    # Step 7: Check if candidate feature vector is optimal
    if len(B) == 0 or fitness > 0.8:
        B = population
        C.append(B)


    # Step 8: Perform k-point crossover
    crossover_point = np.random.randint(1, len(population)-1)
    population = population[:crossover_point] + B[crossover_point:]


    # Step 9: Perform mutation
    mutation_point = np.random.randint(0, len(population)-1)
    if(population[mutation_point]=='1'):
        population = list(population)
        population[mutation_point] = "0"
        population = "".join(population)
    else:
        population = list(population)
        population[mutation_point] = "1"
        population = "".join(population)
    #population[mutation_point] = tcol[np.random.randint(0, len(population)-1)]


    # Step 10: Update fitness value
    #fitness = get_fitness(population)
    #fitness=0.83
    for i in range(30):
        if(population[i]=='1'):
            features.append(col[i])
    fitness=get_fitness(features)

C

"""### **HILL CLIMBING**"""

tcol =np.delete(col, 30)
# initialize the current feature vector
current_features = np.zeros(30, dtype=bool)
for i in range(10):
    current_features[i] = True

# define the neighborhood function to generate neighboring feature vectors
def neighborhood(current_features):
    neighbors = []
    for i in range(len(current_features)):
        neighbor = np.copy(current_features)
        neighbor[i] = not neighbor[i]
        neighbors.append(neighbor)
    return neighbors

#Mapping encoded features to actual features
def get_feature(cur_feat):
    features=[]
    for i in range(30):
        if(cur_feat[i]==1):
            features.append(tcol[i])
    return features

# define the hill climbing algorithm

def hill_climbing(current_features, get_fitness, neighborhood, get_feature):
    i=0;
    while i<2:
        # find the best neighbor
        neighbors = neighborhood(current_features)
        best_neighbor = None
        for neighbor in neighbors:
            if best_neighbor is None or get_fitness(get_feature(neighbor)) > get_fitness(get_feature(best_neighbor)):
                best_neighbor = neighbor
        # if the best neighbor is worse than the current, return the current
        if get_fitness(get_feature(best_neighbor)) <= get_fitness(get_feature(current_features)):
            return current_features
        # otherwise, move to the best neighbor and continue the search
        current_features = best_neighbor
        i+=1
        print(get_feature(current_features))
# run the hill climbing algorithm to select the best features
best_features = hill_climbing(current_features, get_fitness, neighborhood, get_feature)