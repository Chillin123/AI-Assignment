# -*- coding: utf-8 -*-
"""ai3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_DMHxyNRhlE5mjzOet1Xf-VddwgYp6__
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from google.colab import drive
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
import random
from random import randint

drive.mount('/content/drive')
df=pd.read_csv("/content/drive/MyDrive/Aiass/creditcard.csv")

"""### **DATA PREPROCESSING**"""

#Engineering data
df = df.sample(frac=1)
df_fe = df
col = list(df_fe)
sc = StandardScaler()
amount = df['Amount'].values
df['Amount'] = sc.fit_transform(amount.reshape(-1, 1))
df.dropna()
# for col in columns:
#     df_fe = df_fe.fillna({col: df_fe[col].mean()})
x = np.array(df_fe[col])
y = np.array(df['Class'])

# # X is the feature matrix and y is the target variable
# over = SMOTE(sampling_strategy=0.1)
# under = RandomUnderSampler(sampling_strategy=0.5)
# steps = [('o', over), ('u', under)]
# pipeline = Pipeline(steps=steps)

# cnt2=0
# x_resampled, y_resampled = pipeline.fit_resample(x, y)
# for i in range(len(y_resampled)):
#   if(y_resampled[i]==1):
#     cnt2+=1
# print(cnt2)
# print(x_resampled.shape)
# x_resampled

smote = SMOTE(sampling_strategy=0.2)
x_resampled_orig, y_resampled_orig = smote.fit_resample(x, y)

"""### **RANDOM FOREST(FITNESS FUNCTION)**"""

def get_fitness(features):
    train_size = int(len(x_resampled_orig)*0.3)
    x_resampled_df = pd.DataFrame(x_resampled_orig, columns=['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount','Class'])
    x_resampled_subset = x_resampled_df[features].values
    x_resampled_new =x_resampled_subset
    # x_resampled=np.delete(x_resampled, 29, 1)
    x_test = x_resampled_new[train_size:]
    x_train = x_resampled_new[:train_size]
    y_test = y_resampled_orig[train_size:]
    y_train = y_resampled_orig[:train_size]

    #RF classifier
    rf_clf = RandomForestClassifier(max_depth=3)

    #Train RF classifier
    rf_clf.fit(x_train, y_train)

    # Evaluate the model on testing data
    test_preds = rf_clf.predict(x_test)

    # Step 5: Store the predictions
    preds = test_preds

    # Step 6: Evaluate the predictions using accuracy as the performance metric
    accuracy = accuracy_score(y_test, preds)
    return accuracy

"""### **GENETIC ALGORITHM**"""

tcol =np.delete(col, 30)
B = []
C = []
# Step 4: Generate initial population
np.random.shuffle(tcol)
#population = tcol[:16]

#Encoding
population=""
for i in range(30):
     k = random.randint(0, 1)
     population+= str(k);

# Step 5: Compute candidate feature vector
for i in range(2):


    # Step 6: Generate fitness value
    #fitness = get_fitness(population)
    features=[]
    for i in range(30):
        if(population[i]=='1'):
            features.append(col[i])
    fitness=get_fitness(features)


    # Step 7: Check if candidate feature vector is optimal
    if len(B) == 0 or fitness > 0.8:
        B = population
        C.append(B)


    # Step 8: Perform k-point crossover
    crossover_point = np.random.randint(1, len(population)-1)
    population = population[:crossover_point] + B[crossover_point:]


    # Step 9: Perform mutation
    mutation_point = np.random.randint(0, len(population)-1)
    if(population[mutation_point]=='1'):
        population = list(population)
        population[mutation_point] = "0"
        population = "".join(population)
    else:
        population = list(population)
        population[mutation_point] = "1"
        population = "".join(population)
    #population[mutation_point] = tcol[np.random.randint(0, len(population)-1)]


    # Step 10: Update fitness value
    #fitness = get_fitness(population)
    #fitness=0.83
    for i in range(30):
        if(population[i]=='1'):
            features.append(col[i])
    fitness=get_fitness(features)

C

"""### **HILL CLIMBING**"""

tcol =np.delete(col, 30)
# initialize the current feature vector
current_features = np.zeros(30, dtype=bool)
for i in range(10):
    current_features[i] = True

# define the neighborhood function to generate neighboring feature vectors
def neighborhood(current_features):
    neighbors = []
    for i in range(len(current_features)):
        neighbor = np.copy(current_features)
        neighbor[i] = not neighbor[i]
        neighbors.append(neighbor)
    return neighbors

#Mapping encoded features to actual features
def get_feature(cur_feat):
    features=[]
    for i in range(30):
        if(cur_feat[i]==1):
            features.append(tcol[i])
    return features

# define the hill climbing algorithm

def hill_climbing(current_features, get_fitness, neighborhood, get_feature):
    i=0;
    while i<2:
        # find the best neighbor
        neighbors = neighborhood(current_features)
        best_neighbor = None
        for neighbor in neighbors:
            if best_neighbor is None or get_fitness(get_feature(neighbor)) > get_fitness(get_feature(best_neighbor)):
                best_neighbor = neighbor
        # if the best neighbor is worse than the current, return the current
        if get_fitness(get_feature(best_neighbor)) <= get_fitness(get_feature(current_features)):
            return current_features
        # otherwise, move to the best neighbor and continue the search
        current_features = best_neighbor
        i+=1
        print(get_feature(current_features))
# run the hill climbing algorithm to select the best features
best_features = hill_climbing(current_features, get_fitness, neighborhood, get_feature)

"""## PARTICLE SWARM OPTIMIZATION"""

# Define the PSO parameters
n_particles = 3
max_iterations = 2
inertia_weight = 0.7
cognitive_weight = 1.5
social_weight = 1.5

# Load the clean Credit Card Fraud dataset
# Assume the dataset is loaded into X and y, where X is the feature matrix and y is the target variable

# Step 1: Generate initial population (feature names)
tcol =np.delete(col, 30)
A = tcol
n_features = 30
particle_position = np.zeros((n_particles, n_features), dtype=bool)
particle_velocity = np.zeros((n_particles, n_features), dtype=bool)
particle_best_position = np.zeros((n_particles, n_features), dtype=bool)
global_best_position = np.zeros(n_features, dtype=bool)
global_best_fitness = float('-inf')

#Mapping encoded features to actual features
def get_feature(cur_feat):
    features=[]
    for i in range(30):
        if(cur_feat[i]==1):
            features.append(tcol[i])
    return features

# Step 2: Initialize particles' positions and velocities
for i in range(n_particles):
    np.random.shuffle(A)
    particle_position[i] = np.random.rand(n_features) < 0.5
    particle_velocity[i] = np.random.rand(n_features) < 0.5
    particle_best_position[i] = particle_position[i].copy()

# PSO main loop
for iteration in range(max_iterations):
    # Step 3: Compute fitness for each particle
    particle_fitness = np.zeros(n_particles)
    for i in range(n_particles):
        # Use the selected features in particle_position as the candidate feature vector
        # Train a classifier with these features and compute its accuracy
        # Update particle_fitness[i] with the computed accuracy
        # Note: The higher the accuracy, the better the fitness
        cur_fitness= get_fitness(get_feature(particle_position[i]))
        # Update particle's best position and global best position
        if cur_fitness > particle_fitness[i]:
            particle_best_position[i] = particle_position[i].copy()
            particle_fitness[i]=cur_fitness
        if particle_fitness[i] > global_best_fitness:
            global_best_fitness = particle_fitness[i]
            global_best_position = particle_position[i].copy()

    # Update particles' velocities and positions
    for i in range(n_particles):
        # Update particle's velocity using PSO formula
        particle_velocity[i] = (inertia_weight * particle_velocity[i]
                                + cognitive_weight * np.random.rand(n_features) * (particle_best_position[i] ^particle_position[i])
                                + social_weight * np.random.rand(n_features) * (global_best_position ^particle_position[i]))

        # Update particle's position
        particle_position[i] = particle_position[i] ^ (particle_velocity[i] > 0.5)

# Extract the selected features from the global best position
selected_features = np.array(A)[global_best_position].tolist()

# Output the selected features
print("Selected Features:", selected_features)
